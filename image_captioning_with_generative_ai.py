# -*- coding: utf-8 -*-
"""Image Captioning with Generative AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J5LLlhbnsvFB5ABR6fLAg8kNzpNYX6ML
"""

!pip install torch==2.0.1 torchvision==0.15.2
!pip install transformers==4.30.2
!pip install accelerate==0.20.3
!pip install pillow==9.5.0

from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration
import torch
from google.colab import files

# Upload image(s)
uploaded = files.upload()
image_paths = list(uploaded.keys())

# Open images and convert to RGB
images = [Image.open(path).convert('RGB') for path in image_paths]

# Load processor and model
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

# Preprocess images
# Always pass a list of images and enable padding
inputs = processor(images=images, return_tensors="pt", padding=True)

# Generate captions
outputs = model.generate(**inputs)

# Decode captions
captions = [processor.decode(out, skip_special_tokens=True) for out in outputs]

# Print captions
for path, caption in zip(image_paths, captions):
    print(f"Caption for {path}: {caption}")

import gradio as gr

def caption_image(image):
    inputs = processor(image, return_tensors="pt")
    out = model.generate(**inputs)
    return processor.decode(out[0], skip_special_tokens=True)

demo = gr.Interface(fn=caption_image,
                    inputs=gr.Image(type="pil"),
                    outputs="text",
                    title="BLIP Image Captioning")

demo.launch()